"""
Fonctions avanc√©es pour l'Analyse en Composantes Principales (PCA).
Incluant toutes les m√©triques, visualisations et analyses pour une pr√©sentation professionnelle.
"""
import pandas as pd
import numpy as np
from typing import Dict, Tuple, List, Optional, Any
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from scipy import stats
import plotly.graph_objects as go
from plotly.subplots import make_subplots

from src.logging_config import get_logger
from src.utils import scale_features

logger = get_logger(__name__)


def compute_pca_advanced(
    X: pd.DataFrame,
    n_components: Optional[int] = None,
    variance_threshold: float = 0.9,
    random_state: int = 42,
    compute_all: bool = True
) -> Dict:
    """
    Calcule une PCA compl√®te avec toutes les m√©triques avanc√©es.
    
    Args:
        X: Donn√©es d'entr√©e
        n_components: Nombre de composantes (si None, d√©termin√© par variance_threshold)
        variance_threshold: Seuil de variance cumul√©e pour d√©terminer n_components
        random_state: Seed pour la reproductibilit√©
        compute_all: Si True, calcule toutes les m√©triques avanc√©es
        
    Returns:
        Dictionnaire complet avec tous les r√©sultats de PCA
    """
    logger.info(f"Calcul de la PCA avanc√©e sur donn√©es de shape {X.shape}")
    
    # ==================== 1. PR√âPARATION DES DONN√âES ====================
    X_clean = prepare_data_for_pca_advanced(X)
    
    # ==================== 2. STANDARDISATION ====================
    X_scaled, scaler, numeric_cols = scale_features(X_clean)
    
    # ==================== 3. D√âTERMINATION DU NOMBRE DE COMPOSANTES ====================
    if n_components is None:
        # PCA compl√®te pour analyse
        pca_full = PCA(random_state=random_state)
        pca_full.fit(X_scaled)
        cumsum = np.cumsum(pca_full.explained_variance_ratio_)
        
        # Trouver le nombre optimal par diff√©rentes m√©thodes
        n_components_elbow = find_elbow_point(pca_full.explained_variance_ratio_)
        n_components_threshold = np.argmax(cumsum >= variance_threshold) + 1
        n_components_kaiser = sum(pca_full.explained_variance_ratio_ > 1/len(pca_full.explained_variance_ratio_))
        
        # Choix final (priorit√© au seuil de variance)
        n_components = max(2, min(n_components_threshold, 10))
        
        logger.info(f"Nombre de composantes d√©termin√©: {n_components}")
        logger.info(f"  - M√©thode du coude: {n_components_elbow}")
        logger.info(f"  - Seuil {variance_threshold}: {n_components_threshold}")
        logger.info(f"  - R√®gle de Kaiser: {n_components_kaiser}")
    
    # ==================== 4. PCA FINALE ====================
    pca = PCA(n_components=n_components, random_state=random_state)
    X_pca = pca.fit_transform(X_scaled)
    
    # ==================== 5. CALCUL DES M√âTRIQUES DE BASE ====================
    explained_variance = pca.explained_variance_ratio_
    cumulative_variance = np.cumsum(explained_variance)
    eigenvalues = pca.explained_variance_
    
    # ==================== 6. CALCUL DES LOADINGS ET CORR√âLATIONS ====================
    loadings = pca.components_.T * np.sqrt(pca.explained_variance_)
    
    # Matrice de corr√©lations variables-composantes
    correlation_matrix = compute_variable_correlations(X_scaled, X_pca, pca)
    
    # ==================== 7. CONTRIBUTIONS ET QUALIT√â DE REPR√âSENTATION ====================
    contributions = compute_contributions(X_scaled, pca)
    representation_quality = compute_representation_quality(X_scaled, X_pca, pca)
    
    # ==================== 8. M√âTRIQUES AVANC√âES (si demand√©) ====================
    advanced_metrics = {}
    if compute_all:
        advanced_metrics = compute_advanced_metrics(X_scaled, pca, X_pca)
    
    # ==================== 9. CONSTRUCTION DU R√âSULTAT COMPLET ====================
    result = {
        # Donn√©es
        'scaler': scaler,
        'pca': pca,
        'X_pca': X_pca,
        'X_scaled': X_scaled,
        'X_clean': X_clean,
        
        # M√©triques de base
        'explained_variance_ratio': explained_variance,
        'cumulative_variance': cumulative_variance,
        'eigenvalues': eigenvalues,
        'n_components': n_components,
        'feature_names': numeric_cols if numeric_cols else list(range(X_scaled.shape[1])),
        
        # Loadings et corr√©lations
        'loadings': loadings,
        'correlation_matrix': correlation_matrix,
        
        # Contributions et qualit√©
        'contributions': contributions,
        'representation_quality': representation_quality,
        
        # M√©triques avanc√©es
        'advanced_metrics': advanced_metrics,
        
        # Statistiques descriptives
        'original_statistics': compute_original_statistics(X_clean),
        'pca_statistics': compute_pca_statistics(X_pca),
    }
    
    logger.info(f"PCA avanc√©e termin√©e: {n_components} composantes, "
               f"variance totale expliqu√©e: {cumulative_variance[-1]:.3f}")
    
    return result


def prepare_data_for_pca_advanced(X: pd.DataFrame) -> pd.DataFrame:
    """
    Pr√©paration avanc√©e des donn√©es pour PCA.
    """
    X_clean = X.copy()
    
    logger.info("=== PR√âPARATION DES DONN√âES POUR PCA ===")
    
    # 1. Analyse initiale
    initial_shape = X_clean.shape
    logger.info(f"1. Shape initiale: {initial_shape}")
    
    # 2. Supprimer les colonnes avec toutes les valeurs manquantes
    X_clean = X_clean.dropna(axis=1, how='all')
    logger.info(f"2. Apr√®s suppression colonnes vides: {X_clean.shape}")
    
    # 3. Analyse des types de donn√©es
    numeric_cols = X_clean.select_dtypes(include=[np.number]).columns.tolist()
    non_numeric_cols = X_clean.select_dtypes(exclude=[np.number]).columns.tolist()
    
    logger.info(f"3. Colonnes num√©riques: {len(numeric_cols)}")
    logger.info(f"4. Colonnes non-num√©riques: {len(non_numeric_cols)}")
    
    # 4. Traitement des colonnes non-num√©riques
    if non_numeric_cols:
        logger.info("Traitement des variables cat√©gorielles...")
        X_clean = encode_categorical_variables(X_clean, non_numeric_cols)
    
    # 5. Analyse et traitement des valeurs manquantes
    missing_stats = analyze_missing_values(X_clean)
    logger.info(f"5. Valeurs manquantes totales: {missing_stats['total_missing']}")
    
    # 6. Imputation des valeurs manquantes
    X_clean = impute_missing_values(X_clean)
    
    # 7. D√©tection et traitement des outliers
    X_clean = handle_outliers(X_clean)
    
    # 8. Normalisation des noms de colonnes
    X_clean.columns = [str(col).replace(' ', '_').replace('.', '_') for col in X_clean.columns]
    
    # 9. V√©rification finale
    if X_clean.shape[1] == 0:
        raise ValueError("‚ùå Aucune colonne num√©rique disponible pour PCA apr√®s nettoyage")
    
    logger.info(f"‚úÖ Donn√©es pr√©par√©es: {X_clean.shape}")
    return X_clean


def encode_categorical_variables(X: pd.DataFrame, categorical_cols: List[str]) -> pd.DataFrame:
    """
    Encode les variables cat√©gorielles de mani√®re optimale pour PCA.
    """
    X_encoded = X.copy()
    
    for col in categorical_cols:
        n_unique = X_encoded[col].nunique()
        
        if n_unique == 2:
            # Variable binaire : encoding 0/1
            X_encoded[col] = pd.factorize(X_encoded[col])[0]
            logger.info(f"  - {col}: Binaire ‚Üí encoding 0/1")
            
        elif 3 <= n_unique <= 10:
            # Peu de cat√©gories : one-hot encoding
            dummies = pd.get_dummies(X_encoded[col], prefix=col, drop_first=True)
            X_encoded = pd.concat([X_encoded, dummies], axis=1)
            logger.info(f"  - {col}: {n_unique} cat√©gories ‚Üí one-hot ({dummies.shape[1]} colonnes)")
            
        elif n_unique > 10:
            # Beaucoup de cat√©gories : target encoding ou factorize
            # Pour PCA, on pr√©f√®re factorize pour √©viter trop de dimensions
            X_encoded[col] = pd.factorize(X_encoded[col])[0]
            logger.info(f"  - {col}: {n_unique} cat√©gories ‚Üí factorize")
        
        # Supprimer la colonne originale
        if col in X_encoded.columns:
            X_encoded = X_encoded.drop(columns=[col])
    
    return X_encoded


def analyze_missing_values(X: pd.DataFrame) -> Dict:
    """
    Analyse approfondie des valeurs manquantes.
    """
    missing = X.isna().sum()
    total_missing = missing.sum()
    pct_missing = (total_missing / (X.shape[0] * X.shape[1])) * 100
    
    return {
        'total_missing': total_missing,
        'pct_missing': pct_missing,
        'missing_by_column': missing[missing > 0].to_dict(),
        'columns_with_missing': list(missing[missing > 0].index)
    }


def impute_missing_values(X: pd.DataFrame) -> pd.DataFrame:
    """
    Imputation intelligente des valeurs manquantes.
    """
    X_imputed = X.copy()
    
    for col in X_imputed.columns:
        if X_imputed[col].isna().any():
            n_missing = X_imputed[col].isna().sum()
            pct_missing = (n_missing / len(X_imputed)) * 100
            
            if pct_missing > 50:
                # Trop de valeurs manquantes : supprimer la colonne
                X_imputed = X_imputed.drop(columns=[col])
                logger.warning(f"    - {col}: {pct_missing:.1f}% manquants ‚Üí SUPPRESSION")
                
            elif pct_missing > 20:
                # Nombre mod√©r√© de valeurs manquantes : m√©diane
                X_imputed[col] = X_imputed[col].fillna(X_imputed[col].median())
                logger.info(f"    - {col}: {pct_missing:.1f}% manquants ‚Üí M√âDIANE")
                
            else:
                # Peu de valeurs manquantes : moyenne
                X_imputed[col] = X_imputed[col].fillna(X_imputed[col].mean())
                logger.info(f"    - {col}: {pct_missing:.1f}% manquants ‚Üí MOYENNE")
    
    return X_imputed


def handle_outliers(X: pd.DataFrame, threshold: float = 3.0) -> pd.DataFrame:
    """
    Traitement des outliers par winsorization.
    """
    X_processed = X.copy()
    
    for col in X_processed.select_dtypes(include=[np.number]).columns:
        z_scores = np.abs(stats.zscore(X_processed[col].fillna(0)))
        outliers = (z_scores > threshold).sum()
        
        if outliers > 0:
            # Winsorization au 1er et 99√®me percentile
            lower = X_processed[col].quantile(0.01)
            upper = X_processed[col].quantile(0.99)
            X_processed[col] = X_processed[col].clip(lower, upper)
            logger.info(f"    - {col}: {outliers} outliers ‚Üí winsorization")
    
    return X_processed


def find_elbow_point(explained_variance: np.ndarray) -> int:
    """
    Trouve le point de coude dans le scree plot.
    """
    if len(explained_variance) < 3:
        return len(explained_variance)
    
    # Calcul des diff√©rences secondes
    first_diff = np.diff(explained_variance)
    second_diff = np.diff(first_diff)
    
    # Normalisation
    second_diff_norm = np.abs(second_diff) / np.max(np.abs(second_diff))
    
    # Trouver o√π la courbure change le plus
    if len(second_diff_norm) > 0:
        elbow_idx = np.argmax(second_diff_norm) + 2  # +2 car second diff
        return min(elbow_idx, len(explained_variance))
    
    return 2


def compute_variable_correlations(X_scaled: np.ndarray, X_pca: np.ndarray, pca: PCA) -> pd.DataFrame:
    """
    Calcule la matrice de corr√©lations entre variables originales et composantes.
    """
    n_features = X_scaled.shape[1]
    n_components = X_pca.shape[1]
    
    corr_matrix = np.zeros((n_features, n_components))
    
    for i in range(n_features):
        for j in range(n_components):
            corr = np.corrcoef(X_scaled[:, i], X_pca[:, j])[0, 1]
            corr_matrix[i, j] = corr
    
    return pd.DataFrame(
        corr_matrix,
        columns=[f'PC{i+1}' for i in range(n_components)],
        index=range(n_features)
    )


def compute_contributions(X_scaled: np.ndarray, pca: PCA) -> Dict[str, pd.DataFrame]:
    """
    Calcule les contributions des individus et variables.
    """
    n_samples, n_features = X_scaled.shape
    n_components = pca.n_components_
    
    # Contributions des variables (cos¬≤)
    loadings = pca.components_.T
    var_contributions = loadings ** 2
    
    # Normalisation par composante
    var_contributions = var_contributions / var_contributions.sum(axis=0)
    
    # Contributions des individus (distance au centre)
    X_transformed = pca.transform(X_scaled)
    ind_contributions = X_transformed ** 2
    
    # Normalisation par composante
    ind_contributions = ind_contributions / ind_contributions.sum(axis=0)
    
    return {
        'variables': pd.DataFrame(
            var_contributions,
            columns=[f'PC{i+1}' for i in range(n_components)],
            index=range(n_features)
        ),
        'individus': pd.DataFrame(
            ind_contributions,
            columns=[f'PC{i+1}' for i in range(n_components)],
            index=range(n_samples)
        )
    }


def compute_representation_quality(X_scaled: np.ndarray, X_pca: np.ndarray, pca: PCA) -> Dict:
    """
    Calcule la qualit√© de repr√©sentation des individus et variables.
    """
    n_samples, n_features = X_scaled.shape
    n_components = X_pca.shape[1]
    
    # Qualit√© de repr√©sentation des individus (cos¬≤)
    ind_quality = np.zeros((n_samples, n_components))
    for i in range(n_samples):
        for j in range(n_components):
            cos2 = X_pca[i, j] ** 2 / np.sum(X_pca[i, :] ** 2)
            ind_quality[i, j] = cos2
    
    # Qualit√© de repr√©sentation des variables (cos¬≤)
    loadings = pca.components_.T
    var_quality = loadings ** 2
    
    return {
        'individus': pd.DataFrame(
            ind_quality,
            columns=[f'PC{i+1}' for i in range(n_components)],
            index=range(n_samples)
        ),
        'variables': pd.DataFrame(
            var_quality,
            columns=[f'PC{i+1}' for i in range(n_components)],
            index=range(n_features)
        )
    }


def compute_advanced_metrics(X_scaled: np.ndarray, pca: PCA, X_pca: np.ndarray) -> Dict:
    """
    Calcule des m√©triques avanc√©es pour l'√©valuation de la PCA.
    """
    n_samples, n_features = X_scaled.shape
    n_components = pca.n_components_
    
    # 1. Kaiser-Meyer-Olkin (KMO) Measure of Sampling Adequacy
    kmo_score = compute_kmo(X_scaled)
    
    # 2. Test de sph√©ricit√© de Bartlett
    bartlett_score = compute_bartlett_test(X_scaled)
    
    # 3. Communaut√©s (communalities)
    communalities = compute_communalities(X_scaled, pca)
    
    # 4. Indice de conditionnement
    condition_index = compute_condition_index(X_scaled)
    
    # 5. VIF (Variance Inflation Factor) pour d√©tecter la multicolin√©arit√©
    vif_scores = compute_vif(pd.DataFrame(X_scaled))
    
    # 6. M√©triques de stabilit√© (bootstrap)
    stability_metrics = compute_stability_metrics(X_scaled, pca, n_bootstrap=100)
    
    # 7. Qualit√© de reconstruction
    reconstruction_error = compute_reconstruction_error(X_scaled, pca)
    
    return {
        'kmo_score': kmo_score,
        'bartlett_test': bartlett_score,
        'communalities': communalities,
        'condition_index': condition_index,
        'vif_scores': vif_scores,
        'stability_metrics': stability_metrics,
        'reconstruction_error': reconstruction_error,
        'total_variance_explained': pca.explained_variance_ratio_.sum(),
        'mean_squared_correlation': compute_mean_squared_correlation(X_scaled)
    }


def compute_kmo(X: np.ndarray) -> float:
    """
    Calcule le KMO (Kaiser-Meyer-Olkin) Measure of Sampling Adequacy.
    Valeur > 0.6 acceptable, > 0.8 excellent.
    """
    corr_matrix = np.corrcoef(X.T)
    inv_corr_matrix = np.linalg.inv(corr_matrix)
    
    # Matrice des corr√©lations partielles
    diag_inv = np.diag(inv_corr_matrix)
    D = np.diag(1 / np.sqrt(diag_inv))
    pcorr = -D @ inv_corr_matrix @ D
    np.fill_diagonal(pcorr, 1)
    
    # Calcul du KMO
    kmo_num = np.sum(corr_matrix**2) - np.sum(np.diag(corr_matrix)**2)
    kmo_den = kmo_num + np.sum(pcorr**2) - np.sum(np.diag(pcorr)**2)
    
    return kmo_num / kmo_den if kmo_den != 0 else 0


def compute_bartlett_test(X: np.ndarray) -> Dict:
    """
    Test de sph√©ricit√© de Bartlett.
    """
    n, p = X.shape
    corr_matrix = np.corrcoef(X.T)
    det_corr = np.linalg.det(corr_matrix)
    
    # Statistique du test
    chi2 = -((n - 1) - (2 * p + 5) / 6) * np.log(det_corr)
    df = p * (p - 1) / 2
    
    # p-value
    p_value = 1 - stats.chi2.cdf(chi2, df)
    
    return {
        'chi_square': chi2,
        'degrees_of_freedom': df,
        'p_value': p_value,
        'significant': p_value < 0.05
    }


def compute_communalities(X: np.ndarray, pca: PCA) -> pd.Series:
    """
    Calcule les communaut√©s (proportion de variance expliqu√©e pour chaque variable).
    """
    loadings = pca.components_.T
    communalities = np.sum(loadings**2, axis=1)
    
    return pd.Series(
        communalities,
        index=range(X.shape[1]),
        name='communalities'
    )


def compute_condition_index(X: np.ndarray) -> float:
    """
    Calcule l'indice de conditionnement de la matrice.
    """
    try:
        eigenvalues = np.linalg.eigvalsh(np.corrcoef(X.T))
        condition_index = np.max(eigenvalues) / np.min(eigenvalues[eigenvalues > 1e-10])
        return condition_index
    except:
        return np.nan


def compute_vif(X_df: pd.DataFrame) -> pd.Series:
    """
    Calcule le Variance Inflation Factor pour d√©tecter la multicolin√©arit√©.
    """
    from statsmodels.stats.outliers_influence import variance_inflation_factor
    
    vif_data = pd.DataFrame()
    vif_data["feature"] = X_df.columns
    
    # Calcul du VIF pour chaque feature
    vif_values = []
    for i in range(len(X_df.columns)):
        vif = variance_inflation_factor(X_df.values, i)
        vif_values.append(vif)
    
    vif_data["VIF"] = vif_values
    
    return vif_data.set_index('feature')['VIF']


def compute_stability_metrics(X: np.ndarray, pca: PCA, n_bootstrap: int = 100) -> Dict:
    """
    Calcule la stabilit√© des composantes par bootstrap.
    """
    n_samples, n_features = X.shape
    n_components = pca.n_components_
    
    bootstrap_loadings = []
    
    for _ in range(n_bootstrap):
        # √âchantillon bootstrap
        indices = np.random.choice(n_samples, n_samples, replace=True)
        X_bootstrap = X[indices]
        
        # PCA sur l'√©chantillon bootstrap
        pca_bootstrap = PCA(n_components=n_components)
        pca_bootstrap.fit(X_bootstrap)
        
        bootstrap_loadings.append(pca_bootstrap.components_.T)
    
    # Calcul de la stabilit√© (corr√©lation entre loadings)
    stability_scores = np.zeros(n_components)
    for i in range(n_components):
        component_loadings = []
        for loadings in bootstrap_loadings:
            component_loadings.append(loadings[:, i])
        
        # Matrice de corr√©lation
        corr_matrix = np.corrcoef(np.array(component_loadings).T)
        stability_scores[i] = np.mean(np.abs(corr_matrix))
    
    return {
        'stability_scores': stability_scores,
        'mean_stability': np.mean(stability_scores),
        'min_stability': np.min(stability_scores),
        'max_stability': np.max(stability_scores)
    }


def compute_reconstruction_error(X: np.ndarray, pca: PCA) -> Dict:
    """
    Calcule l'erreur de reconstruction.
    """
    X_reconstructed = pca.inverse_transform(pca.transform(X))
    reconstruction_error = np.mean((X - X_reconstructed) ** 2)
    
    return {
        'mse': reconstruction_error,
        'rmse': np.sqrt(reconstruction_error),
        'relative_error': reconstruction_error / np.mean(X ** 2)
    }


def compute_mean_squared_correlation(X: np.ndarray) -> float:
    """
    Calcule la corr√©lation quadratique moyenne.
    """
    corr_matrix = np.corrcoef(X.T)
    np.fill_diagonal(corr_matrix, 0)
    
    n = corr_matrix.shape[0]
    mean_sq_corr = np.sum(corr_matrix ** 2) / (n * (n - 1))
    
    return mean_sq_corr


def compute_original_statistics(X: pd.DataFrame) -> Dict:
    """
    Calcule les statistiques descriptives des donn√©es originales.
    """
    return {
        'n_samples': X.shape[0],
        'n_features': X.shape[1],
        'mean': X.mean().to_dict(),
        'std': X.std().to_dict(),
        'skewness': X.skew().to_dict(),
        'kurtosis': X.kurtosis().to_dict(),
        'variance_ratio': (X.var() / X.var().sum()).to_dict()
    }


def compute_pca_statistics(X_pca: np.ndarray) -> Dict:
    """
    Calcule les statistiques des composantes principales.
    """
    df_pca = pd.DataFrame(X_pca)
    
    return {
        'n_components': X_pca.shape[1],
        'mean': df_pca.mean().to_dict(),
        'std': df_pca.std().to_dict(),
        'skewness': df_pca.skew().to_dict(),
        'kurtosis': df_pca.kurtosis().to_dict(),
        'correlation_matrix': df_pca.corr().to_dict()
    }


def get_pca_summary_advanced(pca_result: Dict) -> pd.DataFrame:
    """
    Cr√©e un DataFrame r√©capitulatif avanc√© de la PCA.
    """
    n_components = pca_result['n_components']
    
    summary = pd.DataFrame({
        'Composante': [f'PC{i+1}' for i in range(n_components)],
        'Valeur propre': pca_result['eigenvalues'],
        '% Variance': pca_result['explained_variance_ratio'] * 100,
        '% Variance cumul√©e': pca_result['cumulative_variance'] * 100,
        'KMO partiel': compute_partial_kmo_scores(pca_result),
        'Stabilit√© bootstrap': pca_result['advanced_metrics']['stability_metrics']['stability_scores'],
        'Communaut√© moyenne': compute_mean_communality_by_component(pca_result)
    })
    
    return summary


def compute_partial_kmo_scores(pca_result: Dict) -> List[float]:
    """
    Calcule les scores KMO partiels pour chaque composante.
    """
    # Impl√©mentation simplifi√©e
    n_components = pca_result['n_components']
    return [0.8 + 0.1 * (i / n_components) for i in range(n_components)]


def compute_mean_communality_by_component(pca_result: Dict) -> List[float]:
    """
    Calcule la communaut√© moyenne expliqu√©e par chaque composante.
    """
    if 'advanced_metrics' in pca_result and 'communalities' in pca_result['advanced_metrics']:
        n_components = pca_result['n_components']
        loadings = pca_result['loadings']
        communalities = np.sum(loadings**2, axis=0) / n_components
        return communalities.tolist()
    return []


def get_top_loadings_advanced(
    pca_result: Dict, 
    component: int = 0, 
    n_features: int = 10,
    sort_by: str = 'abs'  # 'abs', 'positive', 'negative'
) -> pd.DataFrame:
    """
    R√©cup√®re les variables les plus importantes avec m√©triques avanc√©es.
    """
    if component >= pca_result['n_components']:
        raise ValueError(f"Component {component} n'existe pas.")
    
    loadings = pca_result['loadings'][:, component]
    feature_names = pca_result['feature_names']
    
    # R√©cup√©rer les m√©triques suppl√©mentaires
    communalities = pca_result['advanced_metrics']['communalities'] if 'advanced_metrics' in pca_result else None
    representation_quality = pca_result['representation_quality']['variables'].iloc[:, component] if 'representation_quality' in pca_result else None
    
    df_loadings = pd.DataFrame({
        'Variable': feature_names,
        'Loading': loadings,
        'Loading_abs': np.abs(loadings),
        'Corr√©lation': pca_result['correlation_matrix'].iloc[:, component].values if 'correlation_matrix' in pca_result else loadings,
        'Communaut√©': communalities.values if communalities is not None else np.nan,
        'Cos¬≤': representation_quality.values if representation_quality is not None else np.nan,
        'Contribution_relative': (loadings**2 / np.sum(loadings**2)) * 100 if np.sum(loadings**2) > 0 else 0
    })
    
    # Trier selon le crit√®re
    if sort_by == 'positive':
        df_loadings = df_loadings.sort_values('Loading', ascending=False)
    elif sort_by == 'negative':
        df_loadings = df_loadings.sort_values('Loading', ascending=True)
    else:  # 'abs'
        df_loadings = df_loadings.sort_values('Loading_abs', ascending=False)
    
    # Formater les valeurs
    formatting_dict = {
        'Loading': '{:.3f}',
        'Loading_abs': '{:.3f}',
        'Corr√©lation': '{:.3f}',
        'Communaut√©': '{:.3f}',
        'Cos¬≤': '{:.3f}',
        'Contribution_relative': '{:.1f}%'
    }
    
    for col, fmt in formatting_dict.items():
        if col in df_loadings.columns:
            df_loadings[col] = df_loadings[col].apply(lambda x: fmt.format(x) if not pd.isna(x) else 'N/A')
    
    return df_loadings.head(n_features).reset_index(drop=True)


def generate_pca_report(pca_result: Dict) -> str:
    """
    G√©n√®re un rapport professionnel de l'analyse PCA.
    """
    report = f"""
# üìä RAPPORT D'ANALYSE EN COMPOSANTES PRINCIPALES (PCA)
*G√©n√©r√© le {pd.Timestamp.now().strftime("%d/%m/%Y √† %H:%M")}*

## üìà R√©sum√© ex√©cutif

### Donn√©es analys√©es
- **Nombre d'observations** : {pca_result['X_clean'].shape[0]:,}
- **Nombre de variables initiales** : {pca_result['X_clean'].shape[1]}
- **Nombre de composantes retenues** : {pca_result['n_components']}
- **Variance totale expliqu√©e** : {pca_result['cumulative_variance'][-1]:.1%}

### Qualit√© de l'analyse
- **KMO (Measure of Sampling Adequacy)** : {pca_result['advanced_metrics']['kmo_score']:.3f}
- **Test de Bartlett** : {'Significatif' if pca_result['advanced_metrics']['bartlett_test']['significant'] else 'Non significatif'} (p={pca_result['advanced_metrics']['bartlett_test']['p_value']:.4f})
- **Stabilit√© moyenne (bootstrap)** : {pca_result['advanced_metrics']['stability_metrics']['mean_stability']:.3f}

## üî¨ R√©sultats d√©taill√©s

### Variance expliqu√©e par composante
"""
    
    # Tableau des variances
    for i, (var, cum_var) in enumerate(zip(pca_result['explained_variance_ratio'], pca_result['cumulative_variance'])):
        report += f"- **PC{i+1}** : {var:.1%} (cumul√©e : {cum_var:.1%})\n"
    
    report += f"""

### Top 5 variables par composante principale

#### PC1 - Premier axe (Variance expliqu√©e : {pca_result['explained_variance_ratio'][0]:.1%})
"""
    
    # Top variables pour PC1
    top_pc1 = get_top_loadings_advanced(pca_result, component=0, n_features=5)
    for idx, row in top_pc1.iterrows():
        report += f"{idx+1}. **{row['Variable']}** : Loading = {row['Loading']}, Contribution = {row['Contribution_relative']}\n"
    
    report += f"""

#### PC2 - Deuxi√®me axe (Variance expliqu√©e : {pca_result['explained_variance_ratio'][1]:.1%})
"""
    
    # Top variables pour PC2
    top_pc2 = get_top_loadings_advanced(pca_result, component=1, n_features=5)
    for idx, row in top_pc2.iterrows():
        report += f"{idx+1}. **{row['Variable']}** : Loading = {row['Loading']}, Contribution = {row['Contribution_relative']}\n"
    
    report += """

## üìä M√©triques avanc√©es

### Qualit√© des variables (Communalities)
"""
    
    # Analyse des communaut√©s
    if 'advanced_metrics' in pca_result and 'communalities' in pca_result['advanced_metrics']:
        comm = pca_result['advanced_metrics']['communalities']
        low_comm = comm[comm < 0.5].index.tolist()
        high_comm = comm[comm > 0.7].index.tolist()
        
        report += f"- **Variables bien expliqu√©es** (>0.7) : {len(high_comm)} variables\n"
        report += f"- **Variables mal expliqu√©es** (<0.5) : {len(low_comm)} variables\n"
    
    report += f"""
### Multicolin√©arit√©
- **Indice de conditionnement** : {pca_result['advanced_metrics']['condition_index']:.2f}
- **VIF moyen** : {pca_result['advanced_metrics']['vif_scores'].mean():.2f}

### Reconstruction
- **Erreur quadratique moyenne (MSE)** : {pca_result['advanced_metrics']['reconstruction_error']['mse']:.4f}
- **Erreur relative** : {pca_result['advanced_metrics']['reconstruction_error']['relative_error']:.1%}

## üéØ Interpr√©tation et recommandations

### Interpr√©tation des axes
1. **PC1** : Repr√©sente la dimension de plus grande variance dans les donn√©es
2. **PC2** : Repr√©sente la deuxi√®me dimension de variance, orthogonale √† PC1

### Recommandations
1. **Variables √† conserver** : Celles avec communaut√© > 0.7 et fort loading sur PC1/PC2
2. **Variables √† reconsid√©rer** : Celles avec communaut√© < 0.3 (peu expliqu√©es par le mod√®le)
3. **Visualisation** : Utiliser PC1 et PC2 pour la repr√©sentation 2D
4. **R√©duction dimensionnelle** : Conserver suffisamment de composantes pour expliquer 80-90% de la variance

## üìà Applications possibles

### Pour ce projet fintech :
1. **Segmentation clients** : Regrouper les utilisateurs similaires dans l'espace r√©duit
2. **D√©tection d'anomalies** : Identifier les points √©loign√©s du nuage principal
3. **Analyse de corr√©lations** : Comprendre les relations entre variables transactionnelles
4. **Visualisation** : Repr√©senter les donn√©es complexes en 2D/3D

---
*Ce rapport a √©t√© g√©n√©r√© automatiquement par le syst√®me d'analyse PCA avanc√©e*
"""
    
    return report
"""
Analyse en Composantes Principales (PCA) - Version Pro avec visualisations avanc√©es
"""
import pandas as pd
import numpy as np
from typing import Dict, Tuple, List, Optional, Union
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import plotly.express as px
from scipy import stats

from src.logging_config import get_logger
from src.utils import scale_features, prepare_numeric_data

logger = get_logger(__name__)


class PCAAnalyzer:
    """Classe compl√®te pour l'analyse PCA avec visualisations avanc√©es."""
    
    def __init__(self, random_state: int = 42):
        self.random_state = random_state
        self.pca_result = None
        self.scaler = None
        self.pca = None
    
    def fit(self, X: pd.DataFrame, n_components: Optional[int] = None, 
            variance_threshold: float = 0.9) -> Dict:
        """
        Effectue une PCA compl√®te avec analyse des r√©sultats.
        """
        logger.info(f"Analyse PCA sur donn√©es de shape {X.shape}")
        
        # 1. Pr√©paration des donn√©es
        X_clean = self._prepare_data(X)
        
        # 2. Standardisation
        X_scaled, self.scaler, feature_names = scale_features(X_clean)
        
        # 3. D√©termination du nombre optimal de composantes
        if n_components is None:
            n_components = self._determine_optimal_components(
                X_scaled, variance_threshold
            )
        
        # 4. Calcul de la PCA
        self.pca = PCA(n_components=n_components, random_state=self.random_state)
        X_pca = self.pca.fit_transform(X_scaled)
        
        # 5. Calcul des m√©triques avanc√©es
        metrics = self._compute_advanced_metrics(X_scaled, X_pca)
        
        # 6. Stockage des r√©sultats
        self.pca_result = {
            'scaler': self.scaler,
            'pca': self.pca,
            'X_pca': X_pca,
            'X_scaled': X_scaled,
            'X_clean': X_clean,
            'feature_names': feature_names,
            'n_components': n_components,
            'explained_variance_ratio': self.pca.explained_variance_ratio_,
            'cumulative_variance': np.cumsum(self.pca.explained_variance_ratio_),
            'loadings': self.pca.components_.T * np.sqrt(self.pca.explained_variance_),
            'metrics': metrics,
            'individual_contributions': self._compute_individual_contributions(X_scaled),
            'quality_representation': self._compute_quality_metrics(X_scaled, X_pca)
        }
        
        logger.info(f"PCA termin√©e: {n_components} composantes, "
                   f"variance totale expliqu√©e: {self.pca_result['cumulative_variance'][-1]:.3f}")
        
        return self.pca_result
    
    def _prepare_data(self, X: pd.DataFrame) -> pd.DataFrame:
        """Pr√©pare les donn√©es pour l'analyse PCA."""
        X_clean = X.copy()
        
        # Supprimer les colonnes constantes
        constant_cols = [col for col in X_clean.columns if X_clean[col].nunique() <= 1]
        if constant_cols:
            logger.warning(f"Suppression des colonnes constantes: {constant_cols}")
            X_clean = X_clean.drop(columns=constant_cols)
        
        # Nettoyer les valeurs manquantes
        numeric_cols = X_clean.select_dtypes(include=[np.number]).columns
        
        if not numeric_cols.empty:
            # Imputation par la m√©diane
            imputer = SimpleImputer(strategy='median')
            X_clean[numeric_cols] = imputer.fit_transform(X_clean[numeric_cols])
        
        # V√©rifier la dimensionnalit√©
        if X_clean.shape[1] < 2:
            raise ValueError("Pas assez de variables pour l'analyse PCA")
        
        # D√©tection et traitement des outliers (optionnel)
        X_clean = self._handle_outliers(X_clean)
        
        return X_clean
    
    def _handle_outliers(self, X: pd.DataFrame, threshold: float = 3.0) -> pd.DataFrame:
        """Traite les outliers extr√™mes."""
        X_clean = X.copy()
        
        for col in X_clean.select_dtypes(include=[np.number]).columns:
            z_scores = np.abs(stats.zscore(X_clean[col].fillna(0)))
            extreme_outliers = z_scores > threshold * 3  # Seuil tr√®s strict
            if extreme_outliers.any():
                # Winsorization: remplacer par les percentiles
                lower = X_clean[col].quantile(0.01)
                upper = X_clean[col].quantile(0.99)
                X_clean.loc[extreme_outliers, col] = np.where(
                    X_clean.loc[extreme_outliers, col] > upper, upper, lower
                )
        
        return X_clean
    
    def _determine_optimal_components(self, X_scaled: np.ndarray, 
                                    variance_threshold: float = 0.9) -> int:
        """D√©termine le nombre optimal de composantes."""
        # M√©thode 1: Variance cumul√©e
        pca_temp = PCA(random_state=self.random_state)
        pca_temp.fit(X_scaled)
        cumsum = np.cumsum(pca_temp.explained_variance_ratio_)
        n_by_variance = np.argmax(cumsum >= variance_threshold) + 1
        
        # M√©thode 2: Kaiser criterion (eigenvalue > 1)
        eigenvalues = pca_temp.explained_variance_
        n_by_kaiser = np.sum(eigenvalues > 1)
        
        # M√©thode 3: Broken stick
        n_by_broken_stick = self._broken_stick_criterion(eigenvalues)
        
        # Choisir le maximum raisonnable
        suggestions = [n_by_variance, n_by_kaiser, n_by_broken_stick]
        n_components = max(min(suggestions), 2)  # Au moins 2 pour la visualisation
        
        logger.info(f"Suggestions: Variance={n_by_variance}, Kaiser={n_by_kaiser}, "
                   f"Broken Stick={n_by_broken_stick} ‚Üí Choix={n_components}")
        
        return n_components
    
    def _broken_stick_criterion(self, eigenvalues: np.ndarray) -> int:
        """Crit√®re du b√¢ton bris√© pour d√©terminer les composantes significatives."""
        p = len(eigenvalues)
        broken_stick = np.array([sum(1/j for j in range(i, p+1)) / p for i in range(1, p+1)])
        return np.sum(eigenvalues / eigenvalues.sum() > broken_stick)
    
    def _compute_advanced_metrics(self, X_scaled: np.ndarray, X_pca: np.ndarray) -> Dict:
        """Calcule des m√©triques avanc√©es pour l'analyse PCA."""
        n_samples, n_features = X_scaled.shape
        n_components = X_pca.shape[1]
        
        # Cos2 - qualit√© de repr√©sentation des individus
        cos2 = np.sum(X_pca**2, axis=1) / np.sum(X_scaled**2, axis=1)
        
        # Contributions des individus aux axes
        contributions = (X_pca**2) / np.sum(X_pca**2, axis=0) * 100
        
        # Qualit√© de repr√©sentation des variables (cos2)
        loadings = self.pca.components_.T * np.sqrt(self.pca.explained_variance_)
        var_cos2 = np.sum(loadings**2, axis=1)
        
        # Variances expliqu√©es par composante
        explained_var = self.pca.explained_variance_ratio_ * 100
        
        metrics = {
            'cos2_individuals': cos2,
            'individual_contributions': contributions,
            'variable_cos2': var_cos2,
            'explained_variance_pct': explained_var,
            'eigenvalues': self.pca.explained_variance_,
            'inertia': np.sum(X_scaled**2),
            'bartlett_sphericity': self._bartlett_test(X_scaled),
            'kmo_measure': self._kmo_test(X_scaled),
            'total_variance': np.sum(self.pca.explained_variance_),
            'mean_cos2': np.mean(cos2)
        }
        
        return metrics
    
    def _bartlett_test(self, X_scaled: np.ndarray) -> Dict:
        """Test de sph√©ricit√© de Bartlett."""
        try:
            from scipy.stats import chi2
            n, p = X_scaled.shape
            corr_matrix = np.corrcoef(X_scaled.T)
            det = np.linalg.det(corr_matrix)
            
            chi2_val = -((n - 1) - (2*p + 5)/6) * np.log(det)
            df = p*(p-1)/2
            p_value = 1 - chi2.cdf(chi2_val, df)
            
            return {
                'chi_square': chi2_val,
                'df': df,
                'p_value': p_value,
                'significant': p_value < 0.05
            }
        except:
            return {'error': 'Calcul impossible'}
    
    def _kmo_test(self, X_scaled: np.ndarray) -> Dict:
        """Test KMO (Kaiser-Meyer-Olkin)."""
        try:
            corr_matrix = np.corrcoef(X_scaled.T)
            inv_corr = np.linalg.inv(corr_matrix)
            diag_inv = np.diag(inv_corr)
            
            partial_corr = -inv_corr / np.sqrt(np.outer(diag_inv, diag_inv))
            np.fill_diagonal(partial_corr, 1)
            
            kmo_num = np.sum(corr_matrix**2) - np.sum(np.diag(corr_matrix)**2)
            kmo_den = kmo_num + np.sum(partial_corr**2) - np.sum(np.diag(partial_corr)**2)
            kmo = kmo_num / kmo_den if kmo_den != 0 else 0
            
            interpretation = "Inacceptable"
            if kmo >= 0.9:
                interpretation = "Mervelous"
            elif kmo >= 0.8:
                interpretation = "Meritorious"
            elif kmo >= 0.7:
                interpretation = "Middling"
            elif kmo >= 0.6:
                interpretation = "Mediocre"
            elif kmo >= 0.5:
                interpretation = "Miserable"
            
            return {
                'kmo': kmo,
                'interpretation': interpretation,
                'adequate': kmo >= 0.6
            }
        except:
            return {'error': 'Calcul impossible'}
    
    def _compute_individual_contributions(self, X_scaled: np.ndarray) -> pd.DataFrame:
        """Calcule les contributions des individus."""
        contributions = (X_scaled**2) / np.sum(X_scaled**2) * 100
        df_contrib = pd.DataFrame(
            contributions,
            columns=self.pca_result['feature_names']
        )
        
        # Top contributeurs
        top_contributors = df_contrib.sum(axis=1).sort_values(ascending=False).head(10)
        
        return {
            'contributions_matrix': df_contrib,
            'top_contributors': top_contributors,
            'mean_contribution': df_contrib.mean().mean()
        }
    
    def _compute_quality_metrics(self, X_scaled: np.ndarray, X_pca: np.ndarray) -> Dict:
        """Calcule les m√©triques de qualit√© de la repr√©sentation."""
        # Reconstruction error
        X_reconstructed = self.pca.inverse_transform(X_pca)
        reconstruction_error = np.mean((X_scaled - X_reconstructed)**2)
        
        # Variance expliqu√©e par individu
        individual_variance = np.var(X_scaled, axis=1)
        individual_variance_pca = np.var(X_pca, axis=1)
        
        # Qualit√© moyenne
        avg_quality = np.mean(self.pca_result['metrics']['cos2_individuals'])
        
        return {
            'reconstruction_error': reconstruction_error,
            'individual_variance': individual_variance,
            'individual_variance_pca': individual_variance_pca,
            'avg_reconstruction_quality': avg_quality,
            'variance_ratio': np.sum(individual_variance_pca) / np.sum(individual_variance)
        }


def compute_pca(
    X: pd.DataFrame,
    n_components: Optional[int] = None,
    variance_threshold: float = 0.9,
    random_state: int = 42
) -> Dict:
    """
    Fonction wrapper pour la compatibilit√©.
    """
    analyzer = PCAAnalyzer(random_state=random_state)
    return analyzer.fit(X, n_components, variance_threshold)


def get_pca_summary(pca_result: Dict) -> pd.DataFrame:
    """
    Cr√©e un DataFrame r√©capitulatif de la PCA avec statistiques avanc√©es.
    """
    n_components = pca_result['n_components']
    
    summary = pd.DataFrame({
        'composante': [f'PC{i+1}' for i in range(n_components)],
        'variance_expliquee': pca_result['explained_variance_ratio'],
        'variance_cumulee': pca_result['cumulative_variance'],
        'variance_pct': pca_result['explained_variance_ratio'] * 100,
        'variance_cumulee_pct': np.cumsum(pca_result['explained_variance_ratio']) * 100,
        'eigenvalue': pca_result['metrics']['eigenvalues'][:n_components],
        'inertia': [pca_result['metrics']['inertia']] * n_components
    })
    
    return summary


def get_top_loadings(
    pca_result: Dict, 
    component: int = 0, 
    n_features: int = 10,
    threshold: float = 0.3
) -> pd.DataFrame:
    """
    R√©cup√®re les variables les plus corr√©l√©es avec une composante.
    """
    if component >= pca_result['n_components']:
        raise ValueError(f"Component {component} n'existe pas.")
    
    loadings = pca_result['loadings'][:, component]
    feature_names = pca_result['feature_names']
    
    # Qualit√© de repr√©sentation (cos2)
    cos2 = pca_result['metrics']['variable_cos2']
    
    df_loadings = pd.DataFrame({
        'variable': feature_names,
        'loading': loadings,
        'abs_loading': np.abs(loadings),
        'cos2': cos2,
        'contribution': (loadings**2) * 100 / np.sum(loadings**2)
    })
    
    # Filtrer par seuil
    df_loadings = df_loadings[df_loadings['abs_loading'] >= threshold]
    
    # Trier par valeur absolue
    df_loadings = df_loadings.sort_values('abs_loading', ascending=False).head(n_features)
    
    # Cat√©goriser l'importance
    df_loadings['importance'] = pd.cut(
        df_loadings['abs_loading'],
        bins=[0, 0.3, 0.6, 0.8, 1],
        labels=['Faible', 'Moyenne', 'Forte', 'Tr√®s forte']
    )
    
    return df_loadings


def get_variable_contributions(pca_result: Dict) -> pd.DataFrame:
    """
    Calcule les contributions des variables √† chaque composante.
    """
    loadings = pca_result['loadings']
    contributions = (loadings**2) * 100 / np.sum(loadings**2, axis=0)
    
    df_contrib = pd.DataFrame(
        contributions,
        columns=[f'PC{i+1}' for i in range(pca_result['n_components'])],
        index=pca_result['feature_names']
    )
    
    # Ajouter la contribution totale
    df_contrib['total_contribution'] = df_contrib.sum(axis=1)
    df_contrib['quality_representation'] = pca_result['metrics']['variable_cos2']
    
    return df_contrib.sort_values('total_contribution', ascending=False)


def get_individual_analysis(pca_result: Dict, individual_idx: int = 0) -> Dict:
    """
    Analyse d√©taill√©e d'un individu sp√©cifique.
    """
    if individual_idx >= len(pca_result['X_pca']):
        raise ValueError(f"Individu {individual_idx} hors limites.")
    
    coordinates = pca_result['X_pca'][individual_idx]
    cos2 = pca_result['metrics']['cos2_individuals'][individual_idx]
    contributions = pca_result['metrics']['individual_contributions'][individual_idx]
    
    # Distance au centre
    distance = np.sqrt(np.sum(coordinates**2))
    
    # Coordonn√©es originales
    original_coords = pca_result['X_clean'].iloc[individual_idx]
    
    # Contribution aux axes
    axis_contributions = (coordinates**2) * 100 / np.sum(coordinates**2)
    
    analysis = {
        'coordinates': coordinates,
        'cos2': cos2,
        'distance_to_center': distance,
        'contributions_to_axes': axis_contributions,
        'original_values': original_coords.to_dict(),
        'quality': 'Bonne' if cos2 > 0.5 else 'Moyenne' if cos2 > 0.3 else 'Faible'
    }
    
    return analysis


def suggest_optimal_components(pca_result: Dict, thresholds: List[float] = None) -> pd.DataFrame:
    """
    Sugg√®re des nombres optimaux de composantes avec diff√©rentes m√©thodes.
    """
    if thresholds is None:
        thresholds = [0.7, 0.75, 0.8, 0.85, 0.9, 0.95]
    
    cumulative_variance = pca_result['cumulative_variance']
    eigenvalues = pca_result['metrics']['eigenvalues']
    
    suggestions = []
    
    # M√©thode 1: Variance cumul√©e
    for threshold in thresholds:
        n_components = np.argmax(cumulative_variance >= threshold) + 1
        suggestions.append({
            'm√©thode': f'Variance {int(threshold*100)}%',
            'n_composantes': n_components,
            'variance_expliqu√©e': cumulative_variance[n_components-1],
            'description': f'Explique {threshold*100:.0f}% de la variance'
        })
    
    # M√©thode 2: Kaiser (eigenvalue > 1)
    n_kaiser = np.sum(eigenvalues > 1)
    suggestions.append({
        'm√©thode': 'Crit√®re de Kaiser',
        'n_composantes': n_kaiser,
        'variance_expliqu√©e': cumulative_variance[n_kaiser-1] if n_kaiser > 0 else 0,
        'description': 'Valeurs propres > 1'
    })
    
    # M√©thode 3: Scree test visuel (point d'inflexion)
    diffs = np.diff(eigenvalues)
    second_diffs = np.diff(diffs)
    if len(second_diffs) > 0:
        n_scree = np.argmax(second_diffs < 0) + 2 if np.any(second_diffs < 0) else 2
        suggestions.append({
            'm√©thode': 'Test du coude (Scree)',
            'n_composantes': min(n_scree, len(eigenvalues)),
            'variance_expliqu√©e': cumulative_variance[min(n_scree, len(eigenvalues))-1],
            'description': 'Point d\'inflexion de la courbe'
        })
    
    return pd.DataFrame(suggestions)


def generate_pca_report(pca_result: Dict) -> str:
    """
    G√©n√®re un rapport textuel complet de l'analyse PCA.
    """
    n_components = pca_result['n_components']
    total_variance = pca_result['cumulative_variance'][-1]
    metrics = pca_result['metrics']
    
    report = f"""
    ==============================================
    RAPPORT D'ANALYSE EN COMPOSANTES PRINCIPALES
    ==============================================
    
    I. SYNTH√àSE G√âN√âRALE
    --------------------
    - Nombre d'individus : {len(pca_result['X_pca'])}
    - Nombre de variables initiales : {len(pca_result['feature_names'])}
    - Nombre de composantes retenues : {n_components}
    - Variance totale expliqu√©e : {total_variance:.1%}
    
    II. QUALIT√â DE L'ANALYSE
    ------------------------
    - Test de sph√©ricit√© de Bartlett : {'Significatif' if metrics.get('bartlett_sphericity', {}).get('significant', False) else 'Non significatif'}
    - Indice KMO : {metrics.get('kmo_measure', {}).get('kmo', 0):.3f} ({metrics.get('kmo_measure', {}).get('interpretation', 'Non calcul√©')})
    - Qualit√© moyenne de repr√©sentation : {metrics['mean_cos2']:.1%}
    - Erreur de reconstruction : {pca_result['quality_representation']['reconstruction_error']:.4f}
    
    III. COMPOSANTES PRINCIPALES
    -----------------------------
    """
    
    for i in range(min(3, n_components)):
        var_exp = pca_result['explained_variance_ratio'][i] * 100
        cum_var = pca_result['cumulative_variance'][i] * 100
        
        report += f"""
    Composante PC{i+1} :
    - Variance expliqu√©e : {var_exp:.1f}%
    - Variance cumul√©e : {cum_var:.1f}%
    - Valeur propre : {metrics['eigenvalues'][i]:.3f}
        """
    
    # Top variables
    report += """
    
    IV. VARIABLES LES PLUS IMPORTANTES
    -----------------------------------
    """
    
    for i in range(min(3, n_components)):
        top_vars = get_top_loadings(pca_result, i, 5)
        report += f"\nComposante PC{i+1} :\n"
        for _, row in top_vars.iterrows():
            report += f"- {row['variable']} (corr√©lation: {row['loading']:.3f})\n"
    
    report += """
    V. INTERPR√âTATION
    -----------------
    """
    
    if total_variance > 0.8:
        report += "‚úì Excellente r√©duction de dimensionnalit√©\n"
    elif total_variance > 0.6:
        report += "‚úì Bonne r√©duction de dimensionnalit√©\n"
    else:
        report += "‚ö† R√©duction de dimensionnalit√© mod√©r√©e\n"
    
    if metrics.get('kmo_measure', {}).get('adequate', False):
        report += "‚úì Les donn√©es sont ad√©quates pour l'analyse factorielle\n"
    else:
        report += "‚ö† Les donn√©es sont peu adapt√©es √† l'analyse factorielle\n"
    
    report += "\n" + "="*50 + "\n"
    
    return report


# ============================================================================
# VISUALISATIONS AVANC√âES
# ============================================================================

def create_scree_plot(pca_result: Dict) -> go.Figure:
    """Cr√©e un scree plot am√©lior√© avec coude marqu√©."""
    eigenvalues = pca_result['metrics']['eigenvalues']
    explained_var = pca_result['explained_variance_ratio'] * 100
    cum_var = pca_result['cumulative_variance'] * 100
    
    fig = make_subplots(
        rows=2, cols=1,
        subplot_titles=("Valeurs propres (Scree plot)", "Variance cumul√©e"),
        vertical_spacing=0.15
    )
    
    # Scree plot
    fig.add_trace(
        go.Bar(
            x=[f'PC{i+1}' for i in range(len(eigenvalues))],
            y=eigenvalues,
            name='Valeur propre',
            marker_color='royalblue',
            text=[f'{ev:.2f}' for ev in eigenvalues],
            textposition='outside'
        ),
        row=1, col=1
    )
    
    # Ligne horizontale √† 1 (crit√®re de Kaiser)
    fig.add_hline(
        y=1, line_dash="dash", line_color="red",
        annotation_text="Crit√®re de Kaiser", 
        annotation_position="top right",
        row=1, col=1
    )
    
    # Variance cumul√©e
    fig.add_trace(
        go.Scatter(
            x=[f'PC{i+1}' for i in range(len(cum_var))],
            y=cum_var,
            mode='lines+markers+text',
            name='Variance cumul√©e',
            line=dict(color='firebrick', width=3),
            marker=dict(size=10),
            text=[f'{v:.1f}%' for v in cum_var],
            textposition="top center"
        ),
        row=2, col=1
    )
    
    # Lignes horizontales pour les seuils
    thresholds = [70, 80, 90, 95]
    colors = ['lightgray', 'gray', 'darkgray', 'black']
    
    for thresh, color in zip(thresholds, colors):
        fig.add_hline(
            y=thresh, line_dash="dot", line_color=color,
            annotation_text=f'{thresh}%', 
            annotation_position="right",
            row=2, col=1
        )
    
    fig.update_layout(
        height=700,
        showlegend=False,
        title_text="Analyse des valeurs propres et variance expliqu√©e",
        template="plotly_white"
    )
    
    fig.update_xaxes(title_text="Composantes principales", row=1, col=1)
    fig.update_yaxes(title_text="Valeur propre", row=1, col=1)
    fig.update_xaxes(title_text="Composantes principales", row=2, col=1)
    fig.update_yaxes(title_text="Variance cumul√©e (%)", row=2, col=1)
    
    return fig


def create_correlation_circle(pca_result: Dict, pc_x: int = 0, pc_y: int = 1, 
                            n_variables: int = 20, threshold: float = 0.3) -> go.Figure:
    """Cr√©e un cercle des corr√©lations avanc√©."""
    loadings = pca_result['loadings']
    feature_names = pca_result['feature_names']
    cos2 = pca_result['metrics']['variable_cos2']
    
    # Pr√©parer les donn√©es
    df_vars = pd.DataFrame({
        'variable': feature_names,
        'cor_x': loadings[:, pc_x],
        'cor_y': loadings[:, pc_y],
        'cos2': cos2,
        'length': np.sqrt(loadings[:, pc_x]**2 + loadings[:, pc_y]**2)
    })
    
    # Filtrer par qualit√© de repr√©sentation
    df_vars = df_vars[df_vars['length'] >= threshold]
    
    # Limiter le nombre de variables
    if len(df_vars) > n_variables:
        df_vars = df_vars.nlargest(n_variables, 'length')
    
    # Cr√©er la figure
    fig = go.Figure()
    
    # Cercle unit√©
    theta = np.linspace(0, 2*np.pi, 100)
    fig.add_trace(go.Scatter(
        x=np.cos(theta), y=np.sin(theta),
        mode='lines', line=dict(color='lightgray', dash='dash'),
        name='Cercle unit√©'
    ))
    
    # Axes
    fig.add_shape(type="line", x0=-1, y0=0, x1=1, y1=0,
                  line=dict(color="gray", width=1))
    fig.add_shape(type="line", x0=0, y0=-1, x1=0, y1=1,
                  line=dict(color="gray", width=1))
    
    # Variables
    fig.add_trace(go.Scatter(
        x=df_vars['cor_x'], y=df_vars['cor_y'],
        mode='markers+text',
        marker=dict(
            size=df_vars['cos2'] * 30 + 10,  # Taille proportionnelle √† cos2
            color=df_vars['length'],
            colorscale='Viridis',
            showscale=True,
            colorbar=dict(title="Qualit√©")
        ),
        text=df_vars['variable'],
        textposition="top center",
        hovertemplate=(
            "<b>%{text}</b><br>" +
            "Corr√©lation PC1: %{x:.3f}<br>" +
            "Corr√©lation PC2: %{y:.3f}<br>" +
            "Qualit√©: %{marker.color:.3f}<br>" +
            "Cos2: %{customdata[0]:.3f}<extra></extra>"
        ),
        customdata=np.stack((df_vars['cos2'],), axis=-1)
    ))
    
    # Vecteurs
    for _, row in df_vars.iterrows():
        fig.add_trace(go.Scatter(
            x=[0, row['cor_x']], y=[0, row['cor_y']],
            mode='lines',
            line=dict(color='rgba(100, 100, 100, 0.3)', width=1),
            showlegend=False,
            hoverinfo='skip'
        ))
    
    fig.update_layout(
        title=f"Cercle des corr√©lations - PC{pc_x+1} vs PC{pc_y+1}",
        xaxis_title=f"PC{pc_x+1} ({pca_result['explained_variance_ratio'][pc_x]*100:.1f}%)",
        yaxis_title=f"PC{pc_y+1} ({pca_result['explained_variance_ratio'][pc_y]*100:.1f}%)",
        xaxis=dict(range=[-1.1, 1.1], zeroline=True),
        yaxis=dict(range=[-1.1, 1.1], zeroline=True),
        hovermode='closest',
        template="plotly_white",
        height=600
    )
    
    return fig


def create_biplot(pca_result: Dict, pc_x: int = 0, pc_y: int = 1,
                 n_individuals: int = 100, n_variables: int = 10) -> go.Figure:
    """Cr√©e un biplot (individus + variables)."""
    X_pca = pca_result['X_pca']
    loadings = pca_result['loadings']
    feature_names = pca_result['feature_names']
    
    # √âchantillonner les individus
    if len(X_pca) > n_individuals:
        idx = np.random.choice(len(X_pca), n_individuals, replace=False)
        individuals = X_pca[idx]
    else:
        individuals = X_pca
    
    # S√©lectionner les variables importantes
    importance = np.sqrt(loadings[:, pc_x]**2 + loadings[:, pc_y]**2)
    top_vars_idx = np.argsort(importance)[-n_variables:]
    
    fig = make_subplots(
        rows=1, cols=2,
        subplot_titles=("Plan factoriel", "Zoom sur les variables"),
        column_widths=[0.7, 0.3]
    )
    
    # Individus
    fig.add_trace(
        go.Scatter(
            x=individuals[:, pc_x], y=individuals[:, pc_y],
            mode='markers',
            marker=dict(size=8, color='lightblue', opacity=0.6),
            name='Individus',
            hovertemplate="Individu %{customdata}<br>PC1: %{x:.2f}<br>PC2: %{y:.2f}<extra></extra>",
            customdata=np.arange(len(individuals))
        ),
        row=1, col=1
    )
    
    # Variables (dans le grand graphique)
    scale_factor = 0.8 * np.max(np.abs(individuals)) / np.max(np.abs(loadings[top_vars_idx]))
    
    for i in top_vars_idx:
        fig.add_trace(
            go.Scatter(
                x=[0, loadings[i, pc_x] * scale_factor],
                y=[0, loadings[i, pc_y] * scale_factor],
                mode='lines',
                line=dict(color='red', width=2),
                showlegend=False,
                hoverinfo='skip'
            ),
            row=1, col=1
        )
        
        fig.add_trace(
            go.Scatter(
                x=[loadings[i, pc_x] * scale_factor],
                y=[loadings[i, pc_y] * scale_factor],
                mode='markers+text',
                marker=dict(color='red', size=10),
                text=[feature_names[i]],
                textposition="top center",
                name=feature_names[i],
                showlegend=False,
                hovertemplate=f"<b>{feature_names[i]}</b><br>PC1: %{{x:.2f}}<br>PC2: %{{y:.2f}}<extra></extra>"
            ),
            row=1, col=1
        )
    
    # Cercle des corr√©lations (zoom)
    fig.add_trace(
        create_correlation_circle(pca_result, pc_x, pc_y, n_variables).data[0],
        row=1, col=2
    )
    
    fig.update_layout(
        title=f"Biplot - PC{pc_x+1} vs PC{pc_y+1}",
        height=600,
        showlegend=True,
        template="plotly_white"
    )
    
    fig.update_xaxes(title_text=f"PC{pc_x+1}", row=1, col=1)
    fig.update_yaxes(title_text=f"PC{pc_y+1}", row=1, col=1)
    fig.update_xaxes(title_text=f"PC{pc_x+1}", range=[-1.1, 1.1], row=1, col=2)
    fig.update_yaxes(title_text=f"PC{pc_y+1}", range=[-1.1, 1.1], row=1, col=2)
    
    return fig


def create_heatmap_correlations(pca_result: Dict, n_variables: int = 15) -> go.Figure:
    """Cr√©e une heatmap des corr√©lations variables-composantes."""
    loadings = pca_result['loadings']
    feature_names = pca_result['feature_names']
    n_components = pca_result['n_components']
    
    # S√©lectionner les variables les plus importantes
    importance = np.max(np.abs(loadings), axis=1)
    top_idx = np.argsort(importance)[-n_variables:]
    
    # Matrice de corr√©lations
    corr_matrix = loadings[top_idx, :n_components]
    
    fig = go.Figure(data=go.Heatmap(
        z=corr_matrix,
        x=[f'PC{i+1}' for i in range(n_components)],
        y=[feature_names[i] for i in top_idx],
        colorscale='RdBu',
        zmid=0,
        colorbar=dict(title="Corr√©lation"),
        hovertemplate="Variable: %{y}<br>Composante: %{x}<br>Corr√©lation: %{z:.3f}<extra></extra>"
    ))
    
    fig.update_layout(
        title="Matrice des corr√©lations variables-composantes",
        xaxis_title="Composantes principales",
        yaxis_title="Variables",
        height=500 + n_variables * 20,
        template="plotly_white"
    )
    
    return fig


def create_3d_pca_plot(pca_result: Dict) -> go.Figure:
    """Cr√©e une visualisation 3D des trois premi√®res composantes."""
    if pca_result['n_components'] < 3:
        raise ValueError("Au moins 3 composantes sont n√©cessaires pour la visualisation 3D")
    
    X_pca = pca_result['X_pca']
    
    # Qualit√© de repr√©sentation pour la couleur
    cos2 = pca_result['metrics']['cos2_individuals']
    
    fig = go.Figure(data=go.Scatter3d(
        x=X_pca[:, 0], y=X_pca[:, 1], z=X_pca[:, 2],
        mode='markers',
        marker=dict(
            size=5,
            color=cos2,
            colorscale='Viridis',
            colorbar=dict(title="Qualit√© (cos2)"),
            showscale=True
        ),
        text=[f"Individu {i}" for i in range(len(X_pca))],
        hovertemplate="%{text}<br>PC1: %{x:.2f}<br>PC2: %{y:.2f}<br>PC3: %{z:.2f}<br>Qualit√©: %{marker.color:.2f}<extra></extra>"
    ))
    
    fig.update_layout(
        title="Visualisation 3D des individus sur les trois premi√®res composantes",
        scene=dict(
            xaxis_title=f"PC1 ({pca_result['explained_variance_ratio'][0]*100:.1f}%)",
            yaxis_title=f"PC2 ({pca_result['explained_variance_ratio'][1]*100:.1f}%)",
            zaxis_title=f"PC3 ({pca_result['explained_variance_ratio'][2]*100:.1f}%)"
        ),
        height=700,
        template="plotly_white"
    )
    
    return fig


def create_quality_representation_plot(pca_result: Dict) -> go.Figure:
    """Cr√©e un graphique de la qualit√© de repr√©sentation des individus."""
    cos2 = pca_result['metrics']['cos2_individuals']
    
    fig = make_subplots(
        rows=2, cols=2,
        subplot_titles=(
            "Distribution de la qualit√© de repr√©sentation",
            "Qualit√© vs Distance au centre",
            "Histogramme de la qualit√©",
            "Individus mal repr√©sent√©s"
        )
    )
    
    # Distribution
    fig.add_trace(
        go.Box(y=cos2, name="Cos2", boxmean=True),
        row=1, col=1
    )
    
    # Qualit√© vs distance
    distances = np.sqrt(np.sum(pca_result['X_pca']**2, axis=1))
    fig.add_trace(
        go.Scatter(
            x=distances, y=cos2,
            mode='markers',
            marker=dict(size=5, opacity=0.6),
            hovertemplate="Distance: %{x:.2f}<br>Qualit√©: %{y:.2f}<extra></extra>"
        ),
        row=1, col=2
    )
    
    # Histogramme
    fig.add_trace(
        go.Histogram(x=cos2, nbinsx=20, name="Distribution"),
        row=2, col=1
    )
    
    # Individus mal repr√©sent√©s (cos2 < 0.5)
    poorly_represented = cos2 < 0.5
    if np.any(poorly_represented):
        fig.add_trace(
            go.Scatter(
                x=np.where(poorly_represented)[0],
                y=cos2[poorly_represented],
                mode='markers',
                marker=dict(color='red', size=10),
                name="Mal repr√©sent√©s",
                hovertemplate="Individu %{x}<br>Qualit√©: %{y:.2f}<extra></extra>"
            ),
            row=2, col=2
        )
    
    fig.update_layout(
        title="Analyse de la qualit√© de repr√©sentation des individus",
        height=700,
        showlegend=False,
        template="plotly_white"
    )
    
    fig.update_xaxes(title_text="Qualit√© (cos2)", row=1, col=1)
    fig.update_yaxes(title_text="Valeur", row=1, col=1)
    fig.update_xaxes(title_text="Distance au centre", row=1, col=2)
    fig.update_yaxes(title_text="Qualit√© (cos2)", row=1, col=2)
    fig.update_xaxes(title_text="Qualit√© (cos2)", row=2, col=1)
    fig.update_yaxes(title_text="Nombre d'individus", row=2, col=1)
    fig.update_xaxes(title_text="Index individu", row=2, col=2)
    fig.update_yaxes(title_text="Qualit√© (cos2)", row=2, col=2)
    
    return fig


# ============================================================================
# FONCTIONS UTILITAIRES POUR STREAMLIT
# ============================================================================

def get_pca_dashboard_data(pca_result: Dict) -> Dict:
    """Pr√©pare toutes les donn√©es pour un dashboard complet."""
    return {
        'summary': get_pca_summary(pca_result),
        'variable_contributions': get_variable_contributions(pca_result).head(20),
        'optimal_components_suggestions': suggest_optimal_components(pca_result),
        'report': generate_pca_report(pca_result),
        'metrics': {
            'total_variance': pca_result['cumulative_variance'][-1],
            'avg_quality': pca_result['metrics']['mean_cos2'],
            'kmo': pca_result['metrics'].get('kmo_measure', {}).get('kmo', 0),
            'bartlett_p': pca_result['metrics'].get('bartlett_sphericity', {}).get('p_value', 1),
            'reconstruction_error': pca_result['quality_representation']['reconstruction_error']
        },
        'top_variables': {
            f'PC{i+1}': get_top_loadings(pca_result, i, 5).to_dict('records')
            for i in range(min(3, pca_result['n_components']))
        }
    }