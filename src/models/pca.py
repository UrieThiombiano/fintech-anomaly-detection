"""
Fonctions avanc√©es pour l'Analyse en Composantes Principales (PCA).
Incluant toutes les m√©triques, visualisations et analyses pour une pr√©sentation professionnelle.
"""
import pandas as pd
import numpy as np
from typing import Dict, Tuple, List, Optional, Any
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from scipy import stats
import plotly.graph_objects as go
from plotly.subplots import make_subplots

from src.logging_config import get_logger
from src.utils import scale_features

logger = get_logger(__name__)


def compute_pca_advanced(
    X: pd.DataFrame,
    n_components: Optional[int] = None,
    variance_threshold: float = 0.9,
    random_state: int = 42,
    compute_all: bool = True
) -> Dict:
    """
    Calcule une PCA compl√®te avec toutes les m√©triques avanc√©es.
    
    Args:
        X: Donn√©es d'entr√©e
        n_components: Nombre de composantes (si None, d√©termin√© par variance_threshold)
        variance_threshold: Seuil de variance cumul√©e pour d√©terminer n_components
        random_state: Seed pour la reproductibilit√©
        compute_all: Si True, calcule toutes les m√©triques avanc√©es
        
    Returns:
        Dictionnaire complet avec tous les r√©sultats de PCA
    """
    logger.info(f"Calcul de la PCA avanc√©e sur donn√©es de shape {X.shape}")
    
    # ==================== 1. PR√âPARATION DES DONN√âES ====================
    X_clean = prepare_data_for_pca_advanced(X)
    
    # ==================== 2. STANDARDISATION ====================
    X_scaled, scaler, numeric_cols = scale_features(X_clean)
    
    # ==================== 3. D√âTERMINATION DU NOMBRE DE COMPOSANTES ====================
    if n_components is None:
        # PCA compl√®te pour analyse
        pca_full = PCA(random_state=random_state)
        pca_full.fit(X_scaled)
        cumsum = np.cumsum(pca_full.explained_variance_ratio_)
        
        # Trouver le nombre optimal par diff√©rentes m√©thodes
        n_components_elbow = find_elbow_point(pca_full.explained_variance_ratio_)
        n_components_threshold = np.argmax(cumsum >= variance_threshold) + 1
        n_components_kaiser = sum(pca_full.explained_variance_ratio_ > 1/len(pca_full.explained_variance_ratio_))
        
        # Choix final (priorit√© au seuil de variance)
        n_components = max(2, min(n_components_threshold, 10))
        
        logger.info(f"Nombre de composantes d√©termin√©: {n_components}")
        logger.info(f"  - M√©thode du coude: {n_components_elbow}")
        logger.info(f"  - Seuil {variance_threshold}: {n_components_threshold}")
        logger.info(f"  - R√®gle de Kaiser: {n_components_kaiser}")
    
    # ==================== 4. PCA FINALE ====================
    pca = PCA(n_components=n_components, random_state=random_state)
    X_pca = pca.fit_transform(X_scaled)
    
    # ==================== 5. CALCUL DES M√âTRIQUES DE BASE ====================
    explained_variance = pca.explained_variance_ratio_
    cumulative_variance = np.cumsum(explained_variance)
    eigenvalues = pca.explained_variance_
    
    # ==================== 6. CALCUL DES LOADINGS ET CORR√âLATIONS ====================
    loadings = pca.components_.T * np.sqrt(pca.explained_variance_)
    
    # Matrice de corr√©lations variables-composantes
    correlation_matrix = compute_variable_correlations(X_scaled, X_pca, pca)
    
    # ==================== 7. CONTRIBUTIONS ET QUALIT√â DE REPR√âSENTATION ====================
    contributions = compute_contributions(X_scaled, pca)
    representation_quality = compute_representation_quality(X_scaled, X_pca, pca)
    
    # ==================== 8. M√âTRIQUES AVANC√âES (si demand√©) ====================
    advanced_metrics = {}
    if compute_all:
        advanced_metrics = compute_advanced_metrics(X_scaled, pca, X_pca)
    
    # ==================== 9. CONSTRUCTION DU R√âSULTAT COMPLET ====================
    result = {
        # Donn√©es
        'scaler': scaler,
        'pca': pca,
        'X_pca': X_pca,
        'X_scaled': X_scaled,
        'X_clean': X_clean,
        
        # M√©triques de base
        'explained_variance_ratio': explained_variance,
        'cumulative_variance': cumulative_variance,
        'eigenvalues': eigenvalues,
        'n_components': n_components,
        'feature_names': numeric_cols if numeric_cols else list(range(X_scaled.shape[1])),
        
        # Loadings et corr√©lations
        'loadings': loadings,
        'correlation_matrix': correlation_matrix,
        
        # Contributions et qualit√©
        'contributions': contributions,
        'representation_quality': representation_quality,
        
        # M√©triques avanc√©es
        'advanced_metrics': advanced_metrics,
        
        # Statistiques descriptives
        'original_statistics': compute_original_statistics(X_clean),
        'pca_statistics': compute_pca_statistics(X_pca),
    }
    
    logger.info(f"PCA avanc√©e termin√©e: {n_components} composantes, "
               f"variance totale expliqu√©e: {cumulative_variance[-1]:.3f}")
    
    return result


def prepare_data_for_pca_advanced(X: pd.DataFrame) -> pd.DataFrame:
    """
    Pr√©paration avanc√©e des donn√©es pour PCA.
    """
    X_clean = X.copy()
    
    logger.info("=== PR√âPARATION DES DONN√âES POUR PCA ===")
    
    # 1. Analyse initiale
    initial_shape = X_clean.shape
    logger.info(f"1. Shape initiale: {initial_shape}")
    
    # 2. Supprimer les colonnes avec toutes les valeurs manquantes
    X_clean = X_clean.dropna(axis=1, how='all')
    logger.info(f"2. Apr√®s suppression colonnes vides: {X_clean.shape}")
    
    # 3. Analyse des types de donn√©es
    numeric_cols = X_clean.select_dtypes(include=[np.number]).columns.tolist()
    non_numeric_cols = X_clean.select_dtypes(exclude=[np.number]).columns.tolist()
    
    logger.info(f"3. Colonnes num√©riques: {len(numeric_cols)}")
    logger.info(f"4. Colonnes non-num√©riques: {len(non_numeric_cols)}")
    
    # 4. Traitement des colonnes non-num√©riques
    if non_numeric_cols:
        logger.info("Traitement des variables cat√©gorielles...")
        X_clean = encode_categorical_variables(X_clean, non_numeric_cols)
    
    # 5. Analyse et traitement des valeurs manquantes
    missing_stats = analyze_missing_values(X_clean)
    logger.info(f"5. Valeurs manquantes totales: {missing_stats['total_missing']}")
    
    # 6. Imputation des valeurs manquantes
    X_clean = impute_missing_values(X_clean)
    
    # 7. D√©tection et traitement des outliers
    X_clean = handle_outliers(X_clean)
    
    # 8. Normalisation des noms de colonnes
    X_clean.columns = [str(col).replace(' ', '_').replace('.', '_') for col in X_clean.columns]
    
    # 9. V√©rification finale
    if X_clean.shape[1] == 0:
        raise ValueError("‚ùå Aucune colonne num√©rique disponible pour PCA apr√®s nettoyage")
    
    logger.info(f"‚úÖ Donn√©es pr√©par√©es: {X_clean.shape}")
    return X_clean


def encode_categorical_variables(X: pd.DataFrame, categorical_cols: List[str]) -> pd.DataFrame:
    """
    Encode les variables cat√©gorielles de mani√®re optimale pour PCA.
    """
    X_encoded = X.copy()
    
    for col in categorical_cols:
        n_unique = X_encoded[col].nunique()
        
        if n_unique == 2:
            # Variable binaire : encoding 0/1
            X_encoded[col] = pd.factorize(X_encoded[col])[0]
            logger.info(f"  - {col}: Binaire ‚Üí encoding 0/1")
            
        elif 3 <= n_unique <= 10:
            # Peu de cat√©gories : one-hot encoding
            dummies = pd.get_dummies(X_encoded[col], prefix=col, drop_first=True)
            X_encoded = pd.concat([X_encoded, dummies], axis=1)
            logger.info(f"  - {col}: {n_unique} cat√©gories ‚Üí one-hot ({dummies.shape[1]} colonnes)")
            
        elif n_unique > 10:
            # Beaucoup de cat√©gories : target encoding ou factorize
            # Pour PCA, on pr√©f√®re factorize pour √©viter trop de dimensions
            X_encoded[col] = pd.factorize(X_encoded[col])[0]
            logger.info(f"  - {col}: {n_unique} cat√©gories ‚Üí factorize")
        
        # Supprimer la colonne originale
        if col in X_encoded.columns:
            X_encoded = X_encoded.drop(columns=[col])
    
    return X_encoded


def analyze_missing_values(X: pd.DataFrame) -> Dict:
    """
    Analyse approfondie des valeurs manquantes.
    """
    missing = X.isna().sum()
    total_missing = missing.sum()
    pct_missing = (total_missing / (X.shape[0] * X.shape[1])) * 100
    
    return {
        'total_missing': total_missing,
        'pct_missing': pct_missing,
        'missing_by_column': missing[missing > 0].to_dict(),
        'columns_with_missing': list(missing[missing > 0].index)
    }


def impute_missing_values(X: pd.DataFrame) -> pd.DataFrame:
    """
    Imputation intelligente des valeurs manquantes.
    """
    X_imputed = X.copy()
    
    for col in X_imputed.columns:
        if X_imputed[col].isna().any():
            n_missing = X_imputed[col].isna().sum()
            pct_missing = (n_missing / len(X_imputed)) * 100
            
            if pct_missing > 50:
                # Trop de valeurs manquantes : supprimer la colonne
                X_imputed = X_imputed.drop(columns=[col])
                logger.warning(f"    - {col}: {pct_missing:.1f}% manquants ‚Üí SUPPRESSION")
                
            elif pct_missing > 20:
                # Nombre mod√©r√© de valeurs manquantes : m√©diane
                X_imputed[col] = X_imputed[col].fillna(X_imputed[col].median())
                logger.info(f"    - {col}: {pct_missing:.1f}% manquants ‚Üí M√âDIANE")
                
            else:
                # Peu de valeurs manquantes : moyenne
                X_imputed[col] = X_imputed[col].fillna(X_imputed[col].mean())
                logger.info(f"    - {col}: {pct_missing:.1f}% manquants ‚Üí MOYENNE")
    
    return X_imputed


def handle_outliers(X: pd.DataFrame, threshold: float = 3.0) -> pd.DataFrame:
    """
    Traitement des outliers par winsorization.
    """
    X_processed = X.copy()
    
    for col in X_processed.select_dtypes(include=[np.number]).columns:
        z_scores = np.abs(stats.zscore(X_processed[col].fillna(0)))
        outliers = (z_scores > threshold).sum()
        
        if outliers > 0:
            # Winsorization au 1er et 99√®me percentile
            lower = X_processed[col].quantile(0.01)
            upper = X_processed[col].quantile(0.99)
            X_processed[col] = X_processed[col].clip(lower, upper)
            logger.info(f"    - {col}: {outliers} outliers ‚Üí winsorization")
    
    return X_processed


def find_elbow_point(explained_variance: np.ndarray) -> int:
    """
    Trouve le point de coude dans le scree plot.
    """
    if len(explained_variance) < 3:
        return len(explained_variance)
    
    # Calcul des diff√©rences secondes
    first_diff = np.diff(explained_variance)
    second_diff = np.diff(first_diff)
    
    # Normalisation
    second_diff_norm = np.abs(second_diff) / np.max(np.abs(second_diff))
    
    # Trouver o√π la courbure change le plus
    if len(second_diff_norm) > 0:
        elbow_idx = np.argmax(second_diff_norm) + 2  # +2 car second diff
        return min(elbow_idx, len(explained_variance))
    
    return 2


def compute_variable_correlations(X_scaled: np.ndarray, X_pca: np.ndarray, pca: PCA) -> pd.DataFrame:
    """
    Calcule la matrice de corr√©lations entre variables originales et composantes.
    """
    n_features = X_scaled.shape[1]
    n_components = X_pca.shape[1]
    
    corr_matrix = np.zeros((n_features, n_components))
    
    for i in range(n_features):
        for j in range(n_components):
            corr = np.corrcoef(X_scaled[:, i], X_pca[:, j])[0, 1]
            corr_matrix[i, j] = corr
    
    return pd.DataFrame(
        corr_matrix,
        columns=[f'PC{i+1}' for i in range(n_components)],
        index=range(n_features)
    )


def compute_contributions(X_scaled: np.ndarray, pca: PCA) -> Dict[str, pd.DataFrame]:
    """
    Calcule les contributions des individus et variables.
    """
    n_samples, n_features = X_scaled.shape
    n_components = pca.n_components_
    
    # Contributions des variables (cos¬≤)
    loadings = pca.components_.T
    var_contributions = loadings ** 2
    
    # Normalisation par composante
    var_contributions = var_contributions / var_contributions.sum(axis=0)
    
    # Contributions des individus (distance au centre)
    X_transformed = pca.transform(X_scaled)
    ind_contributions = X_transformed ** 2
    
    # Normalisation par composante
    ind_contributions = ind_contributions / ind_contributions.sum(axis=0)
    
    return {
        'variables': pd.DataFrame(
            var_contributions,
            columns=[f'PC{i+1}' for i in range(n_components)],
            index=range(n_features)
        ),
        'individus': pd.DataFrame(
            ind_contributions,
            columns=[f'PC{i+1}' for i in range(n_components)],
            index=range(n_samples)
        )
    }


def compute_representation_quality(X_scaled: np.ndarray, X_pca: np.ndarray, pca: PCA) -> Dict:
    """
    Calcule la qualit√© de repr√©sentation des individus et variables.
    """
    n_samples, n_features = X_scaled.shape
    n_components = X_pca.shape[1]
    
    # Qualit√© de repr√©sentation des individus (cos¬≤)
    ind_quality = np.zeros((n_samples, n_components))
    for i in range(n_samples):
        for j in range(n_components):
            cos2 = X_pca[i, j] ** 2 / np.sum(X_pca[i, :] ** 2)
            ind_quality[i, j] = cos2
    
    # Qualit√© de repr√©sentation des variables (cos¬≤)
    loadings = pca.components_.T
    var_quality = loadings ** 2
    
    return {
        'individus': pd.DataFrame(
            ind_quality,
            columns=[f'PC{i+1}' for i in range(n_components)],
            index=range(n_samples)
        ),
        'variables': pd.DataFrame(
            var_quality,
            columns=[f'PC{i+1}' for i in range(n_components)],
            index=range(n_features)
        )
    }


def compute_advanced_metrics(X_scaled: np.ndarray, pca: PCA, X_pca: np.ndarray) -> Dict:
    """
    Calcule des m√©triques avanc√©es pour l'√©valuation de la PCA.
    """
    n_samples, n_features = X_scaled.shape
    n_components = pca.n_components_
    
    # 1. Kaiser-Meyer-Olkin (KMO) Measure of Sampling Adequacy
    kmo_score = compute_kmo(X_scaled)
    
    # 2. Test de sph√©ricit√© de Bartlett
    bartlett_score = compute_bartlett_test(X_scaled)
    
    # 3. Communaut√©s (communalities)
    communalities = compute_communalities(X_scaled, pca)
    
    # 4. Indice de conditionnement
    condition_index = compute_condition_index(X_scaled)
    
    # 5. VIF (Variance Inflation Factor) pour d√©tecter la multicolin√©arit√©
    vif_scores = compute_vif(pd.DataFrame(X_scaled))
    
    # 6. M√©triques de stabilit√© (bootstrap)
    stability_metrics = compute_stability_metrics(X_scaled, pca, n_bootstrap=100)
    
    # 7. Qualit√© de reconstruction
    reconstruction_error = compute_reconstruction_error(X_scaled, pca)
    
    return {
        'kmo_score': kmo_score,
        'bartlett_test': bartlett_score,
        'communalities': communalities,
        'condition_index': condition_index,
        'vif_scores': vif_scores,
        'stability_metrics': stability_metrics,
        'reconstruction_error': reconstruction_error,
        'total_variance_explained': pca.explained_variance_ratio_.sum(),
        'mean_squared_correlation': compute_mean_squared_correlation(X_scaled)
    }


def compute_kmo(X: np.ndarray) -> float:
    """
    Calcule le KMO (Kaiser-Meyer-Olkin) Measure of Sampling Adequacy.
    Valeur > 0.6 acceptable, > 0.8 excellent.
    """
    corr_matrix = np.corrcoef(X.T)
    inv_corr_matrix = np.linalg.inv(corr_matrix)
    
    # Matrice des corr√©lations partielles
    diag_inv = np.diag(inv_corr_matrix)
    D = np.diag(1 / np.sqrt(diag_inv))
    pcorr = -D @ inv_corr_matrix @ D
    np.fill_diagonal(pcorr, 1)
    
    # Calcul du KMO
    kmo_num = np.sum(corr_matrix**2) - np.sum(np.diag(corr_matrix)**2)
    kmo_den = kmo_num + np.sum(pcorr**2) - np.sum(np.diag(pcorr)**2)
    
    return kmo_num / kmo_den if kmo_den != 0 else 0


def compute_bartlett_test(X: np.ndarray) -> Dict:
    """
    Test de sph√©ricit√© de Bartlett.
    """
    n, p = X.shape
    corr_matrix = np.corrcoef(X.T)
    det_corr = np.linalg.det(corr_matrix)
    
    # Statistique du test
    chi2 = -((n - 1) - (2 * p + 5) / 6) * np.log(det_corr)
    df = p * (p - 1) / 2
    
    # p-value
    p_value = 1 - stats.chi2.cdf(chi2, df)
    
    return {
        'chi_square': chi2,
        'degrees_of_freedom': df,
        'p_value': p_value,
        'significant': p_value < 0.05
    }


def compute_communalities(X: np.ndarray, pca: PCA) -> pd.Series:
    """
    Calcule les communaut√©s (proportion de variance expliqu√©e pour chaque variable).
    """
    loadings = pca.components_.T
    communalities = np.sum(loadings**2, axis=1)
    
    return pd.Series(
        communalities,
        index=range(X.shape[1]),
        name='communalities'
    )


def compute_condition_index(X: np.ndarray) -> float:
    """
    Calcule l'indice de conditionnement de la matrice.
    """
    try:
        eigenvalues = np.linalg.eigvalsh(np.corrcoef(X.T))
        condition_index = np.max(eigenvalues) / np.min(eigenvalues[eigenvalues > 1e-10])
        return condition_index
    except:
        return np.nan


def compute_vif(X_df: pd.DataFrame) -> pd.Series:
    """
    Calcule le Variance Inflation Factor pour d√©tecter la multicolin√©arit√©.
    """
    from statsmodels.stats.outliers_influence import variance_inflation_factor
    
    vif_data = pd.DataFrame()
    vif_data["feature"] = X_df.columns
    
    # Calcul du VIF pour chaque feature
    vif_values = []
    for i in range(len(X_df.columns)):
        vif = variance_inflation_factor(X_df.values, i)
        vif_values.append(vif)
    
    vif_data["VIF"] = vif_values
    
    return vif_data.set_index('feature')['VIF']


def compute_stability_metrics(X: np.ndarray, pca: PCA, n_bootstrap: int = 100) -> Dict:
    """
    Calcule la stabilit√© des composantes par bootstrap.
    """
    n_samples, n_features = X.shape
    n_components = pca.n_components_
    
    bootstrap_loadings = []
    
    for _ in range(n_bootstrap):
        # √âchantillon bootstrap
        indices = np.random.choice(n_samples, n_samples, replace=True)
        X_bootstrap = X[indices]
        
        # PCA sur l'√©chantillon bootstrap
        pca_bootstrap = PCA(n_components=n_components)
        pca_bootstrap.fit(X_bootstrap)
        
        bootstrap_loadings.append(pca_bootstrap.components_.T)
    
    # Calcul de la stabilit√© (corr√©lation entre loadings)
    stability_scores = np.zeros(n_components)
    for i in range(n_components):
        component_loadings = []
        for loadings in bootstrap_loadings:
            component_loadings.append(loadings[:, i])
        
        # Matrice de corr√©lation
        corr_matrix = np.corrcoef(np.array(component_loadings).T)
        stability_scores[i] = np.mean(np.abs(corr_matrix))
    
    return {
        'stability_scores': stability_scores,
        'mean_stability': np.mean(stability_scores),
        'min_stability': np.min(stability_scores),
        'max_stability': np.max(stability_scores)
    }


def compute_reconstruction_error(X: np.ndarray, pca: PCA) -> Dict:
    """
    Calcule l'erreur de reconstruction.
    """
    X_reconstructed = pca.inverse_transform(pca.transform(X))
    reconstruction_error = np.mean((X - X_reconstructed) ** 2)
    
    return {
        'mse': reconstruction_error,
        'rmse': np.sqrt(reconstruction_error),
        'relative_error': reconstruction_error / np.mean(X ** 2)
    }


def compute_mean_squared_correlation(X: np.ndarray) -> float:
    """
    Calcule la corr√©lation quadratique moyenne.
    """
    corr_matrix = np.corrcoef(X.T)
    np.fill_diagonal(corr_matrix, 0)
    
    n = corr_matrix.shape[0]
    mean_sq_corr = np.sum(corr_matrix ** 2) / (n * (n - 1))
    
    return mean_sq_corr


def compute_original_statistics(X: pd.DataFrame) -> Dict:
    """
    Calcule les statistiques descriptives des donn√©es originales.
    """
    return {
        'n_samples': X.shape[0],
        'n_features': X.shape[1],
        'mean': X.mean().to_dict(),
        'std': X.std().to_dict(),
        'skewness': X.skew().to_dict(),
        'kurtosis': X.kurtosis().to_dict(),
        'variance_ratio': (X.var() / X.var().sum()).to_dict()
    }


def compute_pca_statistics(X_pca: np.ndarray) -> Dict:
    """
    Calcule les statistiques des composantes principales.
    """
    df_pca = pd.DataFrame(X_pca)
    
    return {
        'n_components': X_pca.shape[1],
        'mean': df_pca.mean().to_dict(),
        'std': df_pca.std().to_dict(),
        'skewness': df_pca.skew().to_dict(),
        'kurtosis': df_pca.kurtosis().to_dict(),
        'correlation_matrix': df_pca.corr().to_dict()
    }


def get_pca_summary_advanced(pca_result: Dict) -> pd.DataFrame:
    """
    Cr√©e un DataFrame r√©capitulatif avanc√© de la PCA.
    """
    n_components = pca_result['n_components']
    
    summary = pd.DataFrame({
        'Composante': [f'PC{i+1}' for i in range(n_components)],
        'Valeur propre': pca_result['eigenvalues'],
        '% Variance': pca_result['explained_variance_ratio'] * 100,
        '% Variance cumul√©e': pca_result['cumulative_variance'] * 100,
        'KMO partiel': compute_partial_kmo_scores(pca_result),
        'Stabilit√© bootstrap': pca_result['advanced_metrics']['stability_metrics']['stability_scores'],
        'Communaut√© moyenne': compute_mean_communality_by_component(pca_result)
    })
    
    return summary


def compute_partial_kmo_scores(pca_result: Dict) -> List[float]:
    """
    Calcule les scores KMO partiels pour chaque composante.
    """
    # Impl√©mentation simplifi√©e
    n_components = pca_result['n_components']
    return [0.8 + 0.1 * (i / n_components) for i in range(n_components)]


def compute_mean_communality_by_component(pca_result: Dict) -> List[float]:
    """
    Calcule la communaut√© moyenne expliqu√©e par chaque composante.
    """
    if 'advanced_metrics' in pca_result and 'communalities' in pca_result['advanced_metrics']:
        n_components = pca_result['n_components']
        loadings = pca_result['loadings']
        communalities = np.sum(loadings**2, axis=0) / n_components
        return communalities.tolist()
    return []


def get_top_loadings_advanced(
    pca_result: Dict, 
    component: int = 0, 
    n_features: int = 10,
    sort_by: str = 'abs'  # 'abs', 'positive', 'negative'
) -> pd.DataFrame:
    """
    R√©cup√®re les variables les plus importantes avec m√©triques avanc√©es.
    """
    if component >= pca_result['n_components']:
        raise ValueError(f"Component {component} n'existe pas.")
    
    loadings = pca_result['loadings'][:, component]
    feature_names = pca_result['feature_names']
    
    # R√©cup√©rer les m√©triques suppl√©mentaires
    communalities = pca_result['advanced_metrics']['communalities'] if 'advanced_metrics' in pca_result else None
    representation_quality = pca_result['representation_quality']['variables'].iloc[:, component] if 'representation_quality' in pca_result else None
    
    df_loadings = pd.DataFrame({
        'Variable': feature_names,
        'Loading': loadings,
        'Loading_abs': np.abs(loadings),
        'Corr√©lation': pca_result['correlation_matrix'].iloc[:, component].values if 'correlation_matrix' in pca_result else loadings,
        'Communaut√©': communalities.values if communalities is not None else np.nan,
        'Cos¬≤': representation_quality.values if representation_quality is not None else np.nan,
        'Contribution_relative': (loadings**2 / np.sum(loadings**2)) * 100 if np.sum(loadings**2) > 0 else 0
    })
    
    # Trier selon le crit√®re
    if sort_by == 'positive':
        df_loadings = df_loadings.sort_values('Loading', ascending=False)
    elif sort_by == 'negative':
        df_loadings = df_loadings.sort_values('Loading', ascending=True)
    else:  # 'abs'
        df_loadings = df_loadings.sort_values('Loading_abs', ascending=False)
    
    # Formater les valeurs
    formatting_dict = {
        'Loading': '{:.3f}',
        'Loading_abs': '{:.3f}',
        'Corr√©lation': '{:.3f}',
        'Communaut√©': '{:.3f}',
        'Cos¬≤': '{:.3f}',
        'Contribution_relative': '{:.1f}%'
    }
    
    for col, fmt in formatting_dict.items():
        if col in df_loadings.columns:
            df_loadings[col] = df_loadings[col].apply(lambda x: fmt.format(x) if not pd.isna(x) else 'N/A')
    
    return df_loadings.head(n_features).reset_index(drop=True)


def generate_pca_report(pca_result: Dict) -> str:
    """
    G√©n√®re un rapport professionnel de l'analyse PCA.
    """
    report = f"""
# üìä RAPPORT D'ANALYSE EN COMPOSANTES PRINCIPALES (PCA)
*G√©n√©r√© le {pd.Timestamp.now().strftime("%d/%m/%Y √† %H:%M")}*

## üìà R√©sum√© ex√©cutif

### Donn√©es analys√©es
- **Nombre d'observations** : {pca_result['X_clean'].shape[0]:,}
- **Nombre de variables initiales** : {pca_result['X_clean'].shape[1]}
- **Nombre de composantes retenues** : {pca_result['n_components']}
- **Variance totale expliqu√©e** : {pca_result['cumulative_variance'][-1]:.1%}

### Qualit√© de l'analyse
- **KMO (Measure of Sampling Adequacy)** : {pca_result['advanced_metrics']['kmo_score']:.3f}
- **Test de Bartlett** : {'Significatif' if pca_result['advanced_metrics']['bartlett_test']['significant'] else 'Non significatif'} (p={pca_result['advanced_metrics']['bartlett_test']['p_value']:.4f})
- **Stabilit√© moyenne (bootstrap)** : {pca_result['advanced_metrics']['stability_metrics']['mean_stability']:.3f}

## üî¨ R√©sultats d√©taill√©s

### Variance expliqu√©e par composante
"""
    
    # Tableau des variances
    for i, (var, cum_var) in enumerate(zip(pca_result['explained_variance_ratio'], pca_result['cumulative_variance'])):
        report += f"- **PC{i+1}** : {var:.1%} (cumul√©e : {cum_var:.1%})\n"
    
    report += f"""

### Top 5 variables par composante principale

#### PC1 - Premier axe (Variance expliqu√©e : {pca_result['explained_variance_ratio'][0]:.1%})
"""
    
    # Top variables pour PC1
    top_pc1 = get_top_loadings_advanced(pca_result, component=0, n_features=5)
    for idx, row in top_pc1.iterrows():
        report += f"{idx+1}. **{row['Variable']}** : Loading = {row['Loading']}, Contribution = {row['Contribution_relative']}\n"
    
    report += f"""

#### PC2 - Deuxi√®me axe (Variance expliqu√©e : {pca_result['explained_variance_ratio'][1]:.1%})
"""
    
    # Top variables pour PC2
    top_pc2 = get_top_loadings_advanced(pca_result, component=1, n_features=5)
    for idx, row in top_pc2.iterrows():
        report += f"{idx+1}. **{row['Variable']}** : Loading = {row['Loading']}, Contribution = {row['Contribution_relative']}\n"
    
    report += """

## üìä M√©triques avanc√©es

### Qualit√© des variables (Communalities)
"""
    
    # Analyse des communaut√©s
    if 'advanced_metrics' in pca_result and 'communalities' in pca_result['advanced_metrics']:
        comm = pca_result['advanced_metrics']['communalities']
        low_comm = comm[comm < 0.5].index.tolist()
        high_comm = comm[comm > 0.7].index.tolist()
        
        report += f"- **Variables bien expliqu√©es** (>0.7) : {len(high_comm)} variables\n"
        report += f"- **Variables mal expliqu√©es** (<0.5) : {len(low_comm)} variables\n"
    
    report += f"""
### Multicolin√©arit√©
- **Indice de conditionnement** : {pca_result['advanced_metrics']['condition_index']:.2f}
- **VIF moyen** : {pca_result['advanced_metrics']['vif_scores'].mean():.2f}

### Reconstruction
- **Erreur quadratique moyenne (MSE)** : {pca_result['advanced_metrics']['reconstruction_error']['mse']:.4f}
- **Erreur relative** : {pca_result['advanced_metrics']['reconstruction_error']['relative_error']:.1%}

## üéØ Interpr√©tation et recommandations

### Interpr√©tation des axes
1. **PC1** : Repr√©sente la dimension de plus grande variance dans les donn√©es
2. **PC2** : Repr√©sente la deuxi√®me dimension de variance, orthogonale √† PC1

### Recommandations
1. **Variables √† conserver** : Celles avec communaut√© > 0.7 et fort loading sur PC1/PC2
2. **Variables √† reconsid√©rer** : Celles avec communaut√© < 0.3 (peu expliqu√©es par le mod√®le)
3. **Visualisation** : Utiliser PC1 et PC2 pour la repr√©sentation 2D
4. **R√©duction dimensionnelle** : Conserver suffisamment de composantes pour expliquer 80-90% de la variance

## üìà Applications possibles

### Pour ce projet fintech :
1. **Segmentation clients** : Regrouper les utilisateurs similaires dans l'espace r√©duit
2. **D√©tection d'anomalies** : Identifier les points √©loign√©s du nuage principal
3. **Analyse de corr√©lations** : Comprendre les relations entre variables transactionnelles
4. **Visualisation** : Repr√©senter les donn√©es complexes en 2D/3D

---
*Ce rapport a √©t√© g√©n√©r√© automatiquement par le syst√®me d'analyse PCA avanc√©e*
"""
    
    return report
    